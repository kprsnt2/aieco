#!/usr/bin/env python3
"""
AIEco - Local Deployment with LMStudio/Ollama
==============================================
Run the full AIEco ecosystem locally with all features:
- Skills (Anthropic-style)
- Agents (Google ADK)
- RAG Pipeline
- MCP Tools
- API Server

Works with LMStudio, Ollama, or any OpenAI-compatible server.
"""

import subprocess
import sys
import os
import time
from pathlib import Path

# Ensure we're in the right directory
SCRIPT_DIR = Path(__file__).parent.absolute()
BACKEND_DIR = SCRIPT_DIR / "backend"


def install_deps():
    """Install required dependencies"""
    try:
        import httpx
    except ImportError:
        print("ğŸ“¦ Installing dependencies...")
        subprocess.run([sys.executable, "-m", "pip", "install", "httpx", "-q"], check=True)
        import httpx
    return httpx


def check_server(url: str, httpx, timeout: int = 5) -> bool:
    """Check if a server is running"""
    try:
        response = httpx.get(f"{url}/models", timeout=timeout)
        return response.status_code == 200
    except:
        return False


def get_available_models(url: str, httpx) -> list:
    """Get list of loaded models"""
    try:
        response = httpx.get(f"{url}/models", timeout=10)
        if response.status_code == 200:
            data = response.json()
            return [m.get("id", "unknown") for m in data.get("data", [])]
    except:
        pass
    return []


def create_env_file(backend: str, base_url: str, model_name: str):
    """Create .env file for the backend"""
    env_content = f"""# AIEco Local Configuration
# Generated by deploy_local.py

# Local inference backend: vllm, ollama, lmstudio
LOCAL_BACKEND={backend}

# LLM URLs
VLLM_BASE_URL={base_url if backend == 'vllm' else 'http://localhost:8000/v1'}
OLLAMA_BASE_URL={base_url if backend == 'ollama' else 'http://localhost:11434/v1'}
LMSTUDIO_BASE_URL={base_url if backend == 'lmstudio' else 'http://localhost:1234/v1'}

# Model name (use the model loaded in LMStudio/Ollama)
LOCAL_MODEL_NAME={model_name}
DEFAULT_MODEL={model_name}

# Disable database/redis for local mode
DATABASE_ENABLED=false
REDIS_ENABLED=false

# JWT secret for local dev
JWT_SECRET_KEY=local-dev-secret-change-in-production

# Debug mode
DEBUG=true
ENVIRONMENT=development

# CORS (allow local frontends)
CORS_ORIGINS=["http://localhost:3000","http://localhost:3001","http://localhost:5173"]
"""
    
    env_path = BACKEND_DIR / ".env"
    with open(env_path, "w") as f:
        f.write(env_content)
    return env_path


def start_backend():
    """Start the FastAPI backend"""
    print("\nğŸš€ Starting AIEco Backend...")
    print(f"   Working directory: {BACKEND_DIR}")
    
    # Install backend requirements
    req_file = BACKEND_DIR / "requirements.txt"
    if req_file.exists():
        print("   Installing backend dependencies...")
        subprocess.run(
            [sys.executable, "-m", "pip", "install", "-r", str(req_file), "-q"],
            cwd=BACKEND_DIR,
            check=False
        )
    
    # Start uvicorn
    cmd = [
        sys.executable, "-m", "uvicorn",
        "app.main:app",
        "--host", "0.0.0.0",
        "--port", "8080",
        "--reload"
    ]
    
    print("   Starting server on http://localhost:8080")
    process = subprocess.Popen(cmd, cwd=BACKEND_DIR)
    return process


def main():
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                              â•‘
â•‘     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—                       â•‘
â•‘    â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•—                      â•‘
â•‘    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘                      â•‘
â•‘    â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘                      â•‘
â•‘    â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•                      â•‘
â•‘    â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•â•â•â•â•â• â•šâ•â•â•â•â•â• â•šâ•â•â•â•â•â•                       â•‘
â•‘                                                              â•‘
â•‘     ğŸ  Local Mode - LMStudio/Ollama Integration              â•‘
â•‘                                                              â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    
    httpx = install_deps()
    
    # Backend selection
    print("ğŸ”§ Select your local inference backend:\n")
    print("   1. LMStudio    (GUI-based, port 1234)")
    print("   2. Ollama      (CLI-based, port 11434)")
    print("   3. Custom URL  (Any OpenAI-compatible server)")
    print("")
    
    choice = input("Enter choice [1-3, default=1]: ").strip() or "1"
    
    backends = {
        "1": ("lmstudio", "http://localhost:1234/v1", "LMStudio"),
        "2": ("ollama", "http://localhost:11434/v1", "Ollama"),
    }
    
    if choice in backends:
        backend, base_url, name = backends[choice]
    else:
        backend = "custom"
        base_url = input("Enter server URL: ").strip()
        name = "Custom"
    
    # Check server
    print(f"\nğŸ” Checking {name} at {base_url}...")
    
    if check_server(base_url, httpx):
        print(f"âœ… {name} is running!")
        
        models = get_available_models(base_url, httpx)
        if models:
            print(f"\nğŸ“¦ Available models:")
            for i, model in enumerate(models[:5], 1):
                print(f"   {i}. {model}")
            
            if len(models) == 1:
                model_name = models[0]
            else:
                model_choice = input(f"\nSelect model [1-{len(models)}, default=1]: ").strip() or "1"
                try:
                    model_name = models[int(model_choice) - 1]
                except:
                    model_name = models[0]
        else:
            model_name = "local-model"
            print("âš ï¸ No models detected. Make sure to load a model in your server.")
    else:
        print(f"âŒ {name} not running at {base_url}")
        print("")
        if backend == "lmstudio":
            print("ğŸ“‹ To start LMStudio:")
            print("   1. Open LMStudio")
            print("   2. Go to 'Local Server' tab")
            print("   3. Load a model (e.g., Qwen 2.5 3B)")
            print("   4. Click 'Start Server'")
        elif backend == "ollama":
            print("ğŸ“‹ To start Ollama:")
            print("   1. Run: ollama serve")
            print("   2. Run: ollama run qwen2.5:3b")
        print("")
        input("Press Enter after starting your server...")
        
        if not check_server(base_url, httpx):
            print("âŒ Server still not available. Please check your setup.")
            sys.exit(1)
        
        model_name = "local-model"
        models = get_available_models(base_url, httpx)
        if models:
            model_name = models[0]
    
    # Create .env
    print(f"\nâœ… Using model: {model_name}")
    env_path = create_env_file(backend, base_url, model_name)
    print(f"âœ… Created: {env_path}")
    
    # Start backend
    process = start_backend()
    
    # Wait for backend
    print("\nâ³ Waiting for backend to start...")
    time.sleep(3)
    
    # Print success
    print("")
    print("=" * 60)
    print("ğŸ‰ AIEco Local Mode Running!")
    print("=" * 60)
    print("")
    print("ğŸ“¡ Services:")
    print(f"   â€¢ Model Server ({name}): {base_url}")
    print(f"   â€¢ AIEco Backend: http://localhost:8080")
    print(f"   â€¢ API Docs: http://localhost:8080/docs")
    print("")
    print("ğŸ”§ Features Available:")
    print("   âœ… Chat Completions")
    print("   âœ… Skills (11 Anthropic + 5 custom)")
    print("   âœ… Agents (ADK framework)")
    print("   âœ… MCP Tools")
    print("   âœ… RAG (if ChromaDB running)")
    print("")
    print("ğŸ’¡ Quick Test:")
    print(f'   curl http://localhost:8080/api/v1/chat/completions \\')
    print(f'     -H "Content-Type: application/json" \\')
    print(f'     -d \'{{"messages": [{{"role": "user", "content": "Hello!"}}]}}\'')
    print("")
    print("ğŸ’¡ Python Test:")
    print("   from openai import OpenAI")
    print("   client = OpenAI(api_key='local', base_url='http://localhost:8080/api/v1')")
    print("   response = client.chat.completions.create(")
    print(f"       model='{model_name}',")
    print("       messages=[{'role': 'user', 'content': 'Hello!'}]")
    print("   )")
    print("")
    print("=" * 60)
    print("ğŸ›‘ Press Ctrl+C to stop")
    print("=" * 60)
    
    try:
        process.wait()
    except KeyboardInterrupt:
        print("\n\nğŸ›‘ Shutting down...")
        process.terminate()
        print("ğŸ‘‹ Goodbye!")


if __name__ == "__main__":
    main()
