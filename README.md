# ğŸš€ AIEco - Private AI Ecosystem

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)
[![Python 3.11+](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.109+-00a393.svg)](https://fastapi.tiangolo.com/)

> **Production-grade, self-hosted AI ecosystem** with GLM-4.7 358B, Anthropic Skills, Google ADK agents, and 2M token context window.

## âš¡ One-Click Deploy

```bash
# Clone and deploy - that's it!
git clone https://github.com/yourusername/aieco.git
cd aieco
python deploy.py
```

The script auto-detects your hardware and deploys the optimal configuration:

| Hardware | Models Deployed | Context |
|----------|----------------|---------|
| 8x MI300X (1.5TB) | GLM-4.7 + MiniMax M2.1 | 2M tokens |
| 4x MI300X (768GB) | GLM-4.7 358B | 1M tokens |
| 1x MI300X (192GB) | Qwen 2.5 72B | 128K tokens |
| RTX 4090 (24GB) | Qwen 2.5 32B | 32K tokens |
| RTX 3060 (12GB) | Qwen 2.5 7B | 8K tokens |

---

## ğŸ“Š Why AIEco?

### ğŸ’° Save 90%+ vs APIs

| Usage | API Cost | AIEco (IndiaAI) | Savings |
|-------|----------|-----------------|---------|
| Solo Dev | $2,000-5,000/mo | $192/mo | **90%+** |
| Team (10) | $30,000+/mo | $576/mo | **97%** |

### ğŸ”’ 100% Privacy

- Your code never leaves your infrastructure
- No training on your data
- GDPR/HIPAA compliant

### ğŸ“ 10x More Context

```
AIEco 2M     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 2,097,152
Claude Code  â–ˆâ–ˆâ–ˆâ–ˆ                                       200,000
GPT-4 Turbo  â–ˆâ–ˆâ–ˆ                                        128,000
Cursor       â–ˆ                                           32,000
```

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         Client Layer                            â”‚
â”‚   [ Chat App ]  [ CLI ]  [ Dashboard ]  [ Python/JS SDK ]       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    FastAPI Backend (:8080)                      â”‚
â”‚        [ Auth ] [ Rate Limit ] [ Model Router ]                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚                    â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
        â–¼                             â–¼    â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  GLM-4.7 358B     â”‚    â”‚  MiniMax M2.1     â”‚
â”‚  Port :8000       â”‚    â”‚  Port :8001       â”‚
â”‚  GPUs 0-3         â”‚    â”‚  GPUs 4-7         â”‚
â”‚  1M-2M Context    â”‚    â”‚  200K Context     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Data Layer                               â”‚
â”‚     [ PostgreSQL ]  [ ChromaDB ]  [ Redis ]                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¯ Features

### Core
| Feature | Description |
|---------|-------------|
| ğŸ§  **Multi-Model** | GLM-4.7 358B + MiniMax M2.1 simultaneously |
| ğŸ“ **2M Context** | Largest context window for coding |
| ğŸ”Œ **OpenAI Compatible** | Drop-in replacement for any OpenAI SDK |
| ğŸ“š **RAG Pipeline** | Document ingestion, vector search, citations |
| ğŸ¤– **ADK Agents** | Google's agent framework (Sequential, Parallel, Loop) |
| ğŸ¯ **Skills** | Anthropic-style skills (16 official + custom) |
| ğŸ› ï¸ **MCP Tools** | File, shell, code execution, database |

### Enterprise
| Feature | Description |
|---------|-------------|
| ğŸ” **Auth** | JWT + API keys with RBAC |
| ğŸ“Š **Observability** | Langfuse tracing, Prometheus metrics |
| ğŸ’¾ **Prompt Caching** | 90% token savings on repeated context |
| âš¡ **Rate Limiting** | Per-user/API-key limits |
| ğŸ“ **Audit Logging** | Complete request/response logs |

---

## ğŸ“¦ Quick Start

### Option 1: One-Click Deploy (Recommended)

```bash
git clone https://github.com/yourusername/aieco.git
cd aieco
python deploy.py
```

### Option 2: Docker Compose

```bash
# Cloud (8x MI300X)
docker-compose -f docker/docker-compose.cloud.yml up -d

# Local (consumer GPU)
docker-compose -f docker/docker-compose.local.yml up -d
```

### Option 3: Manual Setup

```bash
# 1. Start model server
./model-server/max-context-coding.sh

# 2. Start backend
cd backend
pip install -r requirements.txt
uvicorn app.main:app --port 8080

# 3. Start frontend
cd frontend/chat-app
npm install && npm run dev
```

---

## ğŸ–¥ï¸ CLI Integration

### OpenCode / Aider / Continue.dev

```bash
# Set environment variables
export OPENAI_API_BASE="http://localhost:8000/v1"
export OPENAI_API_KEY="sk-local"
export OPENAI_MODEL="glm-4.7"

# Now use any OpenAI-compatible CLI!
opencode
aider
```

### Python SDK

```python
from aieco import AIEcoClient

client = AIEcoClient(api_key="your-key")

# Chat
response = client.chat("Write a sorting algorithm")

# Stream
for chunk in client.chat_stream("Explain recursion"):
    print(chunk, end="")

# RAG Query
results = client.rag_query("How does auth work?")

# Run Agent
result = client.run_agent("code", "Create a REST API")
```

### JavaScript SDK

```javascript
import { AIEcoClient } from '@aieco/sdk';

const client = new AIEcoClient({ apiKey: 'your-key' });

// Chat
const response = await client.chat('Hello!');

// Stream
await client.chatStream('Tell me a story', chunk => {
  process.stdout.write(chunk);
});
```

---

## ğŸ“ Project Structure

```
aieco/
â”œâ”€â”€ deploy.py             # ğŸš€ One-click deployment
â”œâ”€â”€ backend/              # FastAPI backend
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ api/          # REST endpoints
â”‚   â”‚   â”œâ”€â”€ core/         # Auth, LLM, caching
â”‚   â”‚   â”œâ”€â”€ services/     # Business logic
â”‚   â”‚   â””â”€â”€ models/       # Pydantic schemas
â”‚   â””â”€â”€ tests/            # pytest suite
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ chat-app/         # React chat UI
â”‚   â””â”€â”€ dashboard/        # Admin dashboard
â”œâ”€â”€ model-server/         # vLLM configurations
â”‚   â”œâ”€â”€ multi-model-8x.sh # 8x GPU multi-model
â”‚   â””â”€â”€ max-context-coding.sh # Max context
â”œâ”€â”€ skills/               # Anthropic-style skills (16+)
â”‚   â”œâ”€â”€ mcp-builder/      # MCP server creation
â”‚   â”œâ”€â”€ skill-creator/    # Meta skill
â”‚   â”œâ”€â”€ pdf/              # PDF processing
â”‚   â””â”€â”€ ...
â”œâ”€â”€ sdk/
â”‚   â”œâ”€â”€ python/           # pip install aieco
â”‚   â””â”€â”€ javascript/       # npm install @aieco/sdk
â”œâ”€â”€ docs/                 # Documentation
â”‚   â”œâ”€â”€ COST_ANALYSIS.md
â”‚   â”œâ”€â”€ CODING_CLI_SETUP.md
â”‚   â””â”€â”€ MULTI_MODEL_ARCHITECTURE.md
â””â”€â”€ docker/               # Docker configs
```

---

## ğŸ‡®ğŸ‡³ IndiaAI GPU Access

Get 40% subsidized GPU access:

| GPU | Market Rate | IndiaAI Rate |
|-----|-------------|--------------|
| AMD MI300X | $2.40/hr | **$0.80/hr** |
| NVIDIA H100 | $3.00/hr | **$1.20/hr** |

**How to apply:**
1. Visit [indiaai.gov.in/computing](https://indiaai.gov.in)
2. Register as Startup/Researcher/Student
3. Get verified (1-2 days)
4. Access GPUs at subsidized rates!

---

## ğŸ“Š Performance

| Metric | Value |
|--------|-------|
| Model | GLM-4.7 358B FP8 |
| Context Window | 2,097,152 tokens (2M) |
| Throughput | 40-55 tokens/sec |
| Time to First Token | 180ms (cached) |
| Concurrent Users | 100+ |

---

## ğŸ› ï¸ Configuration

### Environment Variables

```bash
# Backend
DATABASE_URL=postgresql://user:pass@localhost/aieco
REDIS_URL=redis://localhost:6379
JWT_SECRET_KEY=your-secret-key

# Model Server
VLLM_MODEL=THUDM/glm-4.7-358b-a16b-fp8
TENSOR_PARALLEL_SIZE=4
MAX_MODEL_LEN=2097152
```

### Model Options

| Model | VRAM | Context | Best For |
|-------|------|---------|----------|
| GLM-4.7 358B | 400GB | 2M | Coding, reasoning |
| MiniMax M2.1 | 243GB | 200K | Fast inference |
| Qwen 2.5 72B | 48GB | 128K | Balanced |
| Qwen 2.5 7B | 8GB | 32K | Local/small GPU |

---

## ğŸ“š Documentation

| Document | Description |
|----------|-------------|
| [COST_ANALYSIS.md](docs/COST_ANALYSIS.md) | Detailed cost comparison |
| [CODING_CLI_SETUP.md](docs/CODING_CLI_SETUP.md) | CLI integration guide |
| [MULTI_MODEL_ARCHITECTURE.md](docs/MULTI_MODEL_ARCHITECTURE.md) | 8x GPU setup |
| [PRIVACY_ANALYSIS.md](docs/PRIVACY_ANALYSIS.md) | Data privacy benefits |
| [SECURITY.md](docs/SECURITY.md) | Security whitepaper |
| [BENCHMARKS.md](docs/BENCHMARKS.md) | Performance metrics |

---

## ğŸ¤ Contributing

Contributions welcome! See [CONTRIBUTING.md](CONTRIBUTING.md).

```bash
# Install dev dependencies
pip install pre-commit
pre-commit install

# Run tests
cd backend && pytest
```

---

## ğŸ“„ License

MIT License - see [LICENSE](LICENSE).

---

## ğŸ™ Credits

- [GLM-4](https://github.com/THUDM/GLM-4) by Tsinghua University
- [MiniMax](https://huggingface.co/MiniMaxAI) for M2.1
- [Anthropic Skills](https://github.com/anthropics/skills) 
- [Google ADK](https://github.com/google/adk-python)
- [vLLM](https://github.com/vllm-project/vllm)

---

**Built with â¤ï¸ for the open-source AI community**

*Star â­ this repo if you find it useful!*
